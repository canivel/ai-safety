# AI Safety Research Ideas

## Overview

This repository consolidates research ideas for mechanistic interpretability projects, primarily inspired by Neel Nanda's blogs, podcasts, and research directions. I'm exploring multiple project ideas before committing to one for deeper investigation.

## Status

Currently in the **exploration phase** - gathering and documenting potential research directions. More ideas will be added as I continue reviewing materials.

## Research Ideas

| Folder | Topic | Status |
|--------|-------|--------|
| [research-idea-1](research-idea-1/) | Mechanistic Decomposition of Chain-of-Thought Self-Correction | Documented |
| *More coming* | *TBD* | Planned |

## Sources

Research ideas are drawn from:
- Neel Nanda's blog posts on [neelnanda.io](https://neelnanda.io)
- MATS research directions and application guidance
- 80,000 Hours podcast episodes on mechanistic interpretability
- "A Pragmatic Vision for Interpretability" and related posts
- Alignment Forum discussions

## Structure

Each research idea folder contains:
- `README.md` - Project overview and hypothesis
- `docs/` - Detailed plans and methodology
- `resources/` - Technical guides and references
- `experiments/` - Notebooks, data, and results (when implemented)

## Goal

Select the most promising research direction based on:
1. Personal interest and curiosity
2. Tractability for self-directed learning
3. Relevance to AI safety
4. Available tooling (Gemma Scope, TransformerLens, etc.)

