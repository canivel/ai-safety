{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# White-Box Interpretability: Looking Inside AI Models\n",
    "\n",
    "## What is \"White-Box\" Interpretability?\n",
    "\n",
    "Imagine you have two ways to understand how a car works:\n",
    "\n",
    "1. **Black-box**: You can only observe inputs (gas pedal, steering wheel) and outputs (speed, direction). You treat the car as a mystery box.\n",
    "\n",
    "2. **White-box**: You can open the hood, see the engine, trace the fuel lines, watch the pistons fire. You understand the *mechanism*.\n",
    "\n",
    "**White-box interpretability** means we open up the neural network and look at:\n",
    "- The actual numbers (activations) flowing through the network\n",
    "- How different parts of the network contribute to outputs\n",
    "- What \"features\" or \"concepts\" the network has learned\n",
    "\n",
    "---\n",
    "\n",
    "## Why Does This Matter for AI Safety?\n",
    "\n",
    "### The Alignment Problem\n",
    "\n",
    "We want AI systems that:\n",
    "1. Do what we actually want (not just what we literally said)\n",
    "2. Are honest about their uncertainty\n",
    "3. Don't deceive us to achieve goals\n",
    "4. Don't have hidden agendas\n",
    "\n",
    "**The problem**: Current AI systems are trained to produce outputs that *look good* to humans. This creates incentives for:\n",
    "\n",
    "- **Sycophancy**: Telling users what they want to hear, not the truth\n",
    "- **Deception**: Hiding true beliefs/capabilities to avoid correction\n",
    "- **Sandbagging**: Performing worse on purpose (e.g., on safety evaluations)\n",
    "\n",
    "### Why White-Box Helps\n",
    "\n",
    "If we can look *inside* the model, we can potentially:\n",
    "- Detect when a model \"knows\" something different from what it says\n",
    "- Find circuits responsible for deceptive behavior\n",
    "- Build monitors that flag concerning internal states\n",
    "- Surgically remove problematic behaviors\n",
    "\n",
    "---\n",
    "\n",
    "## What We'll Cover in This Notebook\n",
    "\n",
    "1. **Understanding Transformer Internals** - What's actually inside these models?\n",
    "2. **Activation Caching** - How to capture the internal states\n",
    "3. **The Residual Stream** - The \"highway\" of information in transformers\n",
    "4. **Logit Lens** - Peeking at predictions before the final layer\n",
    "5. **Linear Probing** - Training simple classifiers on internal states\n",
    "6. **Base vs Instruct Models** - How does RLHF change internal representations?\n",
    "7. **Introduction to SAEs** - Finding interpretable features\n",
    "\n",
    "Each section includes:\n",
    "- Conceptual explanation (what and why)\n",
    "- Mathematical intuition (the key equations)\n",
    "- Working code (hands-on implementation)\n",
    "- Interpretation guidance (what do results mean?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Understanding Transformer Internals\n",
    "\n",
    "## What's Inside a Transformer?\n",
    "\n",
    "Before we can look inside, we need to understand what we're looking at. Let's build intuition from the ground up.\n",
    "\n",
    "### The Big Picture\n",
    "\n",
    "A transformer language model does one thing: **predict the next token**.\n",
    "\n",
    "```\n",
    "Input: \"The cat sat on the\"\n",
    "Output: probability distribution over all possible next tokens\n",
    "        \"mat\": 15%, \"floor\": 12%, \"roof\": 8%, \"dog\": 0.1%, ...\n",
    "```\n",
    "\n",
    "But *how* does it go from text to probabilities? Let's trace the path.\n",
    "\n",
    "### Step 1: Tokenization\n",
    "\n",
    "Text is converted to **tokens** - integer IDs representing pieces of text.\n",
    "\n",
    "```\n",
    "\"The cat sat\" → [464, 3797, 3332]  (example token IDs)\n",
    "```\n",
    "\n",
    "Tokens aren't always whole words:\n",
    "- \"understanding\" might be [\"under\", \"standing\"] or [\"understand\", \"ing\"]\n",
    "- This depends on the tokenizer's vocabulary\n",
    "\n",
    "### Step 2: Embedding\n",
    "\n",
    "Each token ID is converted to a **vector** (list of numbers) called an embedding.\n",
    "\n",
    "```\n",
    "Token 464 (\"The\") → [0.12, -0.34, 0.56, ..., 0.78]  (e.g., 768 numbers)\n",
    "```\n",
    "\n",
    "**Why vectors?** Neural networks operate on continuous numbers, not discrete symbols. The embedding maps discrete tokens into a continuous space where similar tokens are nearby.\n",
    "\n",
    "The embedding dimension (often called `d_model`) is typically:\n",
    "- Small models: 512-768\n",
    "- Medium models: 1024-2048  \n",
    "- Large models: 4096-8192\n",
    "\n",
    "### Step 3: The Residual Stream\n",
    "\n",
    "Here's the key insight that makes transformers interpretable:\n",
    "\n",
    "**The residual stream is a vector that gets progressively updated as it flows through the model.**\n",
    "\n",
    "Think of it like a document being passed around an office:\n",
    "1. Document starts with initial content (the embedding)\n",
    "2. Each department (layer) reads the document and *adds* their notes\n",
    "3. The final document has accumulated contributions from everyone\n",
    "\n",
    "Mathematically:\n",
    "```\n",
    "residual_0 = embedding(token)\n",
    "residual_1 = residual_0 + attention_layer_0(residual_0) + mlp_layer_0(residual_0)\n",
    "residual_2 = residual_1 + attention_layer_1(residual_1) + mlp_layer_1(residual_1)\n",
    "...\n",
    "residual_final = residual_{n-1} + attention_layer_{n-1} + mlp_layer_{n-1}\n",
    "```\n",
    "\n",
    "The **\"+\"** is crucial! Each layer *adds* to what came before, rather than replacing it. This is called a \"residual connection\" (hence \"residual stream\").\n",
    "\n",
    "### Step 4: Attention Layers\n",
    "\n",
    "Attention lets tokens \"talk to each other\". When processing position 5, the model can look back at positions 0-4 to gather relevant information.\n",
    "\n",
    "**Intuition**: To predict what comes after \"The cat sat on the\", the model needs to know:\n",
    "- There's a cat involved (look back at \"cat\")\n",
    "- The cat is sitting (look back at \"sat\")\n",
    "- We're describing a location (look back at \"on\")\n",
    "\n",
    "Attention computes:\n",
    "1. **Query**: \"What am I looking for?\" (from current position)\n",
    "2. **Key**: \"What do I contain?\" (from all previous positions)\n",
    "3. **Value**: \"What information should I send?\" (from all previous positions)\n",
    "\n",
    "The attention pattern shows which positions are \"talking\" to which.\n",
    "\n",
    "### Step 5: MLP Layers\n",
    "\n",
    "MLP (Multi-Layer Perceptron) layers process each position independently. They're thought to store \"facts\" and perform \"lookup\" operations.\n",
    "\n",
    "**Intuition**: \n",
    "- Attention gathers information from context\n",
    "- MLPs retrieve/transform that information based on learned knowledge\n",
    "\n",
    "Example: After attention gathers \"cat\" + \"sat\" + \"on\", the MLP might activate knowledge about \"places cats sit\" → increases probability of \"mat\", \"floor\", \"couch\"\n",
    "\n",
    "### Step 6: Unembedding\n",
    "\n",
    "The final residual stream vector is converted back to token probabilities:\n",
    "\n",
    "```\n",
    "logits = unembed(residual_final)  # Vector of size vocab_size (~50k-250k)\n",
    "probabilities = softmax(logits)    # Convert to probabilities that sum to 1\n",
    "```\n",
    "\n",
    "The unembedding matrix is often the transpose of the embedding matrix (\"weight tying\").\n",
    "\n",
    "---\n",
    "\n",
    "## Visual Summary\n",
    "\n",
    "```\n",
    "Text: \"The cat sat\"\n",
    "        ↓\n",
    "    [Tokenize]\n",
    "        ↓\n",
    "Tokens: [464, 3797, 3332]\n",
    "        ↓\n",
    "    [Embed]\n",
    "        ↓\n",
    "Residual Stream: [[...], [...], [...]]  ← 3 vectors, one per token\n",
    "        ↓\n",
    "    [Layer 0: Attention + MLP]\n",
    "        ↓\n",
    "Residual Stream: [[...], [...], [...]]  ← updated vectors\n",
    "        ↓\n",
    "    [Layer 1: Attention + MLP]\n",
    "        ↓\n",
    "       ...\n",
    "        ↓\n",
    "    [Layer N-1: Attention + MLP]\n",
    "        ↓\n",
    "Final Residual: [[...], [...], [...]]  \n",
    "        ↓\n",
    "    [Unembed]\n",
    "        ↓\n",
    "Logits: [[...50k...], [...50k...], [...50k...]]\n",
    "        ↓\n",
    "    [Softmax]\n",
    "        ↓\n",
    "Probabilities for next token after each position\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens import utils as tl_utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "# Check CUDA\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = Path(\"../results\")\n",
    "FIGURES_DIR = Path(\"../figures\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Load a Model\n",
    "\n",
    "We'll use **Pythia-410M** - a good size for learning (small enough to run easily, large enough to show interesting behaviors).\n",
    "\n",
    "**TransformerLens** is a library specifically designed for interpretability research. It gives us \"hooks\" to capture internal activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"pythia-410m\"\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "# Let's examine the model's architecture\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"MODEL ARCHITECTURE: {MODEL_NAME}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total parameters: {model.cfg.n_params / 1e6:.0f}M\")\n",
    "print(f\"Number of layers: {model.cfg.n_layers}\")\n",
    "print(f\"Hidden dimension (d_model): {model.cfg.d_model}\")\n",
    "print(f\"Number of attention heads: {model.cfg.n_heads}\")\n",
    "print(f\"Head dimension (d_head): {model.cfg.d_head}\")\n",
    "print(f\"MLP hidden dimension (d_mlp): {model.cfg.d_mlp}\")\n",
    "print(f\"Vocabulary size: {model.cfg.d_vocab}\")\n",
    "print(f\"Context window: {model.cfg.n_ctx}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "### Understanding the Architecture Numbers\n",
    "\n",
    "Let's unpack what these numbers mean:\n",
    "\n",
    "- **d_model = 1024**: Each token is represented as a vector of 1024 numbers. This is the \"width\" of the residual stream.\n",
    "\n",
    "- **n_layers = 24**: The model has 24 transformer blocks stacked. More layers = more \"steps\" of computation = potentially more sophisticated reasoning.\n",
    "\n",
    "- **n_heads = 16**: Each attention layer has 16 \"attention heads\" that can attend to different things in parallel. One head might focus on syntax, another on semantics, etc.\n",
    "\n",
    "- **d_head = 64**: Each attention head works in a 64-dimensional subspace. Note: d_model = n_heads × d_head (1024 = 16 × 64).\n",
    "\n",
    "- **d_mlp = 4096**: The MLP hidden layer is 4x the model dimension. This is where a lot of \"knowledge\" is stored.\n",
    "\n",
    "- **d_vocab = 50304**: The model knows ~50k different tokens.\n",
    "\n",
    "- **n_ctx = 2048**: Maximum sequence length the model can process at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Activation Caching - Capturing Internal States\n",
    "\n",
    "## What is Activation Caching?\n",
    "\n",
    "When a neural network runs, it computes many intermediate values:\n",
    "- The residual stream at each layer\n",
    "- Attention patterns (which tokens attend to which)\n",
    "- MLP activations\n",
    "- And more...\n",
    "\n",
    "Normally, these are computed and then discarded (to save memory). **Activation caching** means saving these intermediate values so we can analyze them.\n",
    "\n",
    "**Analogy**: It's like recording a cooking show vs. just eating the final dish. By recording, you can go back and see exactly what happened at each step.\n",
    "\n",
    "## TransformerLens Hooks\n",
    "\n",
    "TransformerLens provides a clean way to cache activations using `run_with_cache()`.\n",
    "\n",
    "The cache is organized by \"hook names\" that follow a pattern:\n",
    "- `hook_embed`: Token embeddings\n",
    "- `blocks.0.hook_resid_pre`: Residual stream before layer 0\n",
    "- `blocks.0.attn.hook_pattern`: Attention pattern in layer 0\n",
    "- `blocks.0.hook_resid_post`: Residual stream after layer 0\n",
    "- `blocks.0.mlp.hook_post`: MLP output in layer 0\n",
    "- And so on...\n",
    "\n",
    "Let's see this in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run a simple example and cache everything\n",
    "example_text = \"The cat sat on the mat.\"\n",
    "\n",
    "# Tokenize first to see what we're working with\n",
    "tokens = model.to_tokens(example_text)\n",
    "print(f\"Input text: '{example_text}'\")\n",
    "print(f\"Token IDs: {tokens[0].tolist()}\")\n",
    "print(f\"Decoded tokens: {[model.to_string(t) for t in tokens[0]]}\")\n",
    "print(f\"Number of tokens: {tokens.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run with caching\n",
    "# This captures ALL intermediate activations\n",
    "logits, cache = model.run_with_cache(tokens)\n",
    "\n",
    "print(f\"Output logits shape: {logits.shape}\")\n",
    "print(f\"  - Batch size: {logits.shape[0]}\")\n",
    "print(f\"  - Sequence length: {logits.shape[1]}\")\n",
    "print(f\"  - Vocabulary size: {logits.shape[2]}\")\n",
    "\n",
    "print(f\"\\nNumber of cached activations: {len(cache)}\")\n",
    "print(f\"\\nSample of cached activation names:\")\n",
    "for i, name in enumerate(list(cache.keys())[:15]):\n",
    "    print(f\"  {name}: shape {cache[name].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### Understanding the Cache\n",
    "\n",
    "Let's look at the most important cached activations:\n",
    "\n",
    "**Residual Stream** (`hook_resid_pre`, `hook_resid_post`):\n",
    "- Shape: `[batch, seq_len, d_model]` = `[1, 8, 1024]`\n",
    "- This is the main \"information highway\"\n",
    "- `resid_pre` = state before a layer processes it\n",
    "- `resid_post` = state after a layer processes it\n",
    "\n",
    "**Attention Patterns** (`attn.hook_pattern`):\n",
    "- Shape: `[batch, n_heads, seq_len, seq_len]` = `[1, 16, 8, 8]`\n",
    "- Shows which tokens attend to which\n",
    "- Entry [h, i, j] = how much head h at position i attends to position j\n",
    "\n",
    "**MLP Activations** (`mlp.hook_post`):\n",
    "- Shape: `[batch, seq_len, d_model]` = `[1, 8, 1024]`\n",
    "- The output of the MLP that gets added to residual stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the residual stream at different layers\n",
    "print(\"Residual Stream Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get residual stream at different points\n",
    "resid_start = cache[\"hook_embed\"]  # After embedding, before any layers\n",
    "resid_mid = cache[\"blocks.12.hook_resid_post\"]  # After layer 12 (middle)\n",
    "resid_end = cache[\"blocks.23.hook_resid_post\"]  # After final layer\n",
    "\n",
    "print(f\"\\nResidual stream at START (after embedding):\")\n",
    "print(f\"  Shape: {resid_start.shape}\")\n",
    "print(f\"  Mean: {resid_start.mean():.4f}\")\n",
    "print(f\"  Std: {resid_start.std():.4f}\")\n",
    "print(f\"  Norm (avg over positions): {resid_start.norm(dim=-1).mean():.4f}\")\n",
    "\n",
    "print(f\"\\nResidual stream at MIDDLE (after layer 12):\")\n",
    "print(f\"  Shape: {resid_mid.shape}\")\n",
    "print(f\"  Mean: {resid_mid.mean():.4f}\")\n",
    "print(f\"  Std: {resid_mid.std():.4f}\")\n",
    "print(f\"  Norm (avg over positions): {resid_mid.norm(dim=-1).mean():.4f}\")\n",
    "\n",
    "print(f\"\\nResidual stream at END (after layer 23):\")\n",
    "print(f\"  Shape: {resid_end.shape}\")\n",
    "print(f\"  Mean: {resid_end.mean():.4f}\")\n",
    "print(f\"  Std: {resid_end.std():.4f}\")\n",
    "print(f\"  Norm (avg over positions): {resid_end.norm(dim=-1).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "### Key Observation: The Residual Stream Grows\n",
    "\n",
    "Notice how the **norm** (length) of the residual stream vectors increases through the layers. This is because each layer *adds* to it.\n",
    "\n",
    "This has important implications:\n",
    "1. Later layers have \"more information\" accumulated\n",
    "2. The final layer has the most \"refined\" representation\n",
    "3. We can peek at intermediate layers to see \"work in progress\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how residual stream norm changes through layers\n",
    "norms_by_layer = []\n",
    "\n",
    "# Start with embeddings\n",
    "norms_by_layer.append(cache[\"hook_embed\"].norm(dim=-1).mean().item())\n",
    "\n",
    "# Then each layer\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    resid = cache[f\"blocks.{layer}.hook_resid_post\"]\n",
    "    norms_by_layer.append(resid.norm(dim=-1).mean().item())\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(range(len(norms_by_layer)), norms_by_layer, 'b-o', linewidth=2, markersize=4)\n",
    "plt.xlabel(\"Layer (0 = embedding)\")\n",
    "plt.ylabel(\"Average Residual Stream Norm\")\n",
    "plt.title(\"How the Residual Stream Grows Through the Model\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"residual_stream_norm.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nNorm increase from start to end: {norms_by_layer[-1] / norms_by_layer[0]:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: The Logit Lens - Peeking at Intermediate Predictions\n",
    "\n",
    "## What is the Logit Lens?\n",
    "\n",
    "Here's a powerful idea: **What if we could see what the model would predict at each intermediate layer?**\n",
    "\n",
    "Normally, only the final residual stream gets converted to predictions (via the unembedding matrix). But we can apply the same unembedding to *any* layer's residual stream!\n",
    "\n",
    "```\n",
    "Layer 0 residual → unembed → \"What would layer 0 predict?\"\n",
    "Layer 5 residual → unembed → \"What would layer 5 predict?\"\n",
    "Layer 12 residual → unembed → \"What would layer 12 predict?\"\n",
    "...\n",
    "Layer 23 residual → unembed → Final prediction (normal output)\n",
    "```\n",
    "\n",
    "## Why is This Useful for Alignment?\n",
    "\n",
    "The logit lens can reveal:\n",
    "\n",
    "1. **When does the model \"make up its mind\"?**\n",
    "   - Early layers: Very uncertain\n",
    "   - Middle layers: Starting to form a prediction\n",
    "   - Late layers: Confident in final answer\n",
    "\n",
    "2. **Does the model \"know\" something it doesn't say?**\n",
    "   - If layer 10 predicts \"X\" but layer 23 predicts \"Y\"\n",
    "   - The model may have \"changed its mind\" due to later processing\n",
    "   - This could indicate deception, sycophancy, or just refinement\n",
    "\n",
    "3. **Where does knowledge get \"retrieved\"?**\n",
    "   - Sudden jumps in confidence often indicate MLP knowledge lookup\n",
    "\n",
    "Let's implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_lens(model, cache, position=-1):\n",
    "    \"\"\"\n",
    "    Apply the logit lens: get predictions from each layer's residual stream.\n",
    "    \n",
    "    Args:\n",
    "        model: The HookedTransformer model\n",
    "        cache: Activation cache from run_with_cache\n",
    "        position: Which token position to analyze (-1 = last)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with predictions at each layer\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for layer in range(model.cfg.n_layers + 1):  # +1 to include post-final-layer\n",
    "        if layer == 0:\n",
    "            # Use embeddings (before any transformer layers)\n",
    "            resid = cache[\"hook_embed\"]\n",
    "            layer_name = \"embed\"\n",
    "        else:\n",
    "            # Use residual stream after this layer\n",
    "            resid = cache[f\"blocks.{layer-1}.hook_resid_post\"]\n",
    "            layer_name = f\"layer_{layer-1}\"\n",
    "        \n",
    "        # Apply layer norm (important for getting good predictions)\n",
    "        resid_normalized = model.ln_final(resid)\n",
    "        \n",
    "        # Get logits by applying unembedding\n",
    "        # resid shape: [batch, seq, d_model]\n",
    "        # unembed shape: [d_model, vocab]\n",
    "        logits = resid_normalized @ model.W_U  # [batch, seq, vocab]\n",
    "        \n",
    "        # Get prediction at specified position\n",
    "        logits_at_pos = logits[0, position, :]  # [vocab]\n",
    "        probs = F.softmax(logits_at_pos, dim=-1)\n",
    "        \n",
    "        # Get top predictions\n",
    "        top_probs, top_indices = probs.topk(5)\n",
    "        top_tokens = [model.to_string(idx) for idx in top_indices]\n",
    "        \n",
    "        results.append({\n",
    "            \"layer\": layer_name,\n",
    "            \"layer_num\": layer,\n",
    "            \"top_token\": top_tokens[0],\n",
    "            \"top_prob\": top_probs[0].item(),\n",
    "            \"top_5\": list(zip(top_tokens, top_probs.tolist())),\n",
    "            \"entropy\": -(probs * probs.log()).sum().item(),  # Uncertainty measure\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Logit lens function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's apply the logit lens to our example\n",
    "# We'll look at what the model predicts after \"The cat sat on the\"\n",
    "\n",
    "example_text = \"The cat sat on the\"\n",
    "tokens = model.to_tokens(example_text)\n",
    "logits, cache = model.run_with_cache(tokens)\n",
    "\n",
    "print(f\"Input: '{example_text}'\")\n",
    "print(f\"Tokens: {[model.to_string(t) for t in tokens[0]]}\")\n",
    "print(f\"\\nAnalyzing predictions after the last token ('the')...\\n\")\n",
    "\n",
    "lens_results = logit_lens(model, cache, position=-1)\n",
    "\n",
    "print(f\"{'Layer':<12} {'Top Prediction':<15} {'Prob':<10} {'Entropy':<10}\")\n",
    "print(\"=\" * 50)\n",
    "for r in lens_results:\n",
    "    print(f\"{r['layer']:<12} {repr(r['top_token']):<15} {r['top_prob']:.4f}     {r['entropy']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### Interpreting the Logit Lens Results\n",
    "\n",
    "Look at how the prediction evolves through layers:\n",
    "\n",
    "1. **Early layers (0-5)**: High entropy, uncertain predictions. The model hasn't \"figured it out\" yet.\n",
    "\n",
    "2. **Middle layers (6-15)**: Entropy starts dropping. The model is converging on an answer.\n",
    "\n",
    "3. **Late layers (16-23)**: Low entropy, confident prediction. The answer is \"locked in\".\n",
    "\n",
    "**Key insight**: If the model were being deceptive, we might see it \"know\" the right answer at layer 10 but then \"change\" to a different answer by layer 23. This would be visible in the logit lens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how predictions evolve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "layers = [r[\"layer_num\"] for r in lens_results]\n",
    "top_probs = [r[\"top_prob\"] for r in lens_results]\n",
    "entropies = [r[\"entropy\"] for r in lens_results]\n",
    "\n",
    "# Plot 1: Top token probability\n",
    "axes[0].plot(layers, top_probs, 'g-o', linewidth=2, markersize=4)\n",
    "axes[0].set_xlabel(\"Layer\")\n",
    "axes[0].set_ylabel(\"Probability of Top Token\")\n",
    "axes[0].set_title(\"Confidence in Top Prediction by Layer\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "# Plot 2: Entropy (uncertainty)\n",
    "axes[1].plot(layers, entropies, 'r-o', linewidth=2, markersize=4)\n",
    "axes[1].set_xlabel(\"Layer\")\n",
    "axes[1].set_ylabel(\"Entropy (bits)\")\n",
    "axes[1].set_title(\"Prediction Uncertainty by Layer\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f\"Logit Lens: '{example_text}' → ?\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"logit_lens_example.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Linear Probing - Training Classifiers on Internal States\n",
    "\n",
    "## What is Linear Probing?\n",
    "\n",
    "Linear probing asks: **Can we train a simple linear classifier to extract information from the model's internal states?**\n",
    "\n",
    "The idea:\n",
    "1. Get activations from some layer of the model\n",
    "2. Train a linear classifier (just weights, no hidden layers) to predict some property\n",
    "3. If the classifier works well, that information is \"linearly encoded\" in the activations\n",
    "\n",
    "**Why \"linear\"?** If a linear probe works, it means the information is stored in a simple, interpretable way - as a direction in the activation space. Non-linear probes can extract more information but are harder to interpret.\n",
    "\n",
    "## Why is This Useful for Alignment?\n",
    "\n",
    "We can train probes to detect:\n",
    "\n",
    "1. **Truth vs. Falsehood**: Does the model \"know\" a statement is false even when it outputs it?\n",
    "2. **Confidence**: Is the model uncertain internally even when it sounds confident?\n",
    "3. **Sycophancy**: Can we detect when the model is \"people-pleasing\" vs. being truthful?\n",
    "4. **User Modeling**: Is there a representation of \"what the user wants to hear\"?\n",
    "\n",
    "## Example: Probing for Sentiment\n",
    "\n",
    "Let's start with a simple example: can we find a \"sentiment direction\" in the model's activations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple dataset of positive and negative sentences\n",
    "positive_texts = [\n",
    "    \"This movie was absolutely wonderful and amazing!\",\n",
    "    \"I love this product, it's fantastic!\",\n",
    "    \"The food was delicious and the service was great.\",\n",
    "    \"What a beautiful day, I feel so happy!\",\n",
    "    \"This is the best experience I've ever had.\",\n",
    "    \"Incredible performance, truly outstanding!\",\n",
    "    \"I'm so grateful for this wonderful opportunity.\",\n",
    "    \"The team did an excellent job on this project.\",\n",
    "]\n",
    "\n",
    "negative_texts = [\n",
    "    \"This movie was terrible and disappointing.\",\n",
    "    \"I hate this product, it's awful!\",\n",
    "    \"The food was disgusting and the service was horrible.\",\n",
    "    \"What a terrible day, I feel so miserable.\",\n",
    "    \"This is the worst experience I've ever had.\",\n",
    "    \"Awful performance, truly disappointing!\",\n",
    "    \"I'm so frustrated with this terrible situation.\",\n",
    "    \"The team did a horrible job on this project.\",\n",
    "]\n",
    "\n",
    "print(f\"Positive examples: {len(positive_texts)}\")\n",
    "print(f\"Negative examples: {len(negative_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_activations(model, texts, layer=-1):\n",
    "    \"\"\"\n",
    "    Get residual stream activations for a list of texts.\n",
    "    \n",
    "    Args:\n",
    "        model: HookedTransformer model\n",
    "        texts: List of strings\n",
    "        layer: Which layer's output to use (-1 = final layer)\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape [n_texts, d_model] - one activation vector per text\n",
    "    \"\"\"\n",
    "    activations = []\n",
    "    \n",
    "    for text in texts:\n",
    "        tokens = model.to_tokens(text)\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "        \n",
    "        # Get residual stream at specified layer\n",
    "        if layer == -1:\n",
    "            layer_idx = model.cfg.n_layers - 1\n",
    "        else:\n",
    "            layer_idx = layer\n",
    "        \n",
    "        resid = cache[f\"blocks.{layer_idx}.hook_resid_post\"]\n",
    "        \n",
    "        # Take the activation at the last token position\n",
    "        # Shape: [batch=1, seq_len, d_model] → [d_model]\n",
    "        act = resid[0, -1, :]\n",
    "        activations.append(act)\n",
    "    \n",
    "    return torch.stack(activations)  # [n_texts, d_model]\n",
    "\n",
    "# Get activations for positive and negative texts\n",
    "print(\"Extracting activations from final layer...\")\n",
    "pos_activations = get_activations(model, positive_texts, layer=-1)\n",
    "neg_activations = get_activations(model, negative_texts, layer=-1)\n",
    "\n",
    "print(f\"Positive activations shape: {pos_activations.shape}\")\n",
    "print(f\"Negative activations shape: {neg_activations.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a simple linear probe\n",
    "# The probe is just: prediction = activations @ probe_direction\n",
    "# We find the direction that best separates positive from negative\n",
    "\n",
    "# Method 1: Difference of means (simplest approach)\n",
    "# The \"sentiment direction\" is just the vector from negative centroid to positive centroid\n",
    "\n",
    "pos_mean = pos_activations.mean(dim=0)  # [d_model]\n",
    "neg_mean = neg_activations.mean(dim=0)  # [d_model]\n",
    "\n",
    "sentiment_direction = pos_mean - neg_mean  # [d_model]\n",
    "sentiment_direction = sentiment_direction / sentiment_direction.norm()  # Normalize\n",
    "\n",
    "print(f\"Sentiment direction shape: {sentiment_direction.shape}\")\n",
    "print(f\"Sentiment direction norm: {sentiment_direction.norm():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's see how well this direction separates the data\n",
    "# Project activations onto the sentiment direction\n",
    "\n",
    "pos_scores = pos_activations @ sentiment_direction  # [n_pos]\n",
    "neg_scores = neg_activations @ sentiment_direction  # [n_neg]\n",
    "\n",
    "print(\"Sentiment Scores (higher = more positive)\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nPositive texts:\")\n",
    "for text, score in zip(positive_texts, pos_scores):\n",
    "    print(f\"  {score:.2f}: {text[:50]}...\")\n",
    "\n",
    "print(\"\\nNegative texts:\")\n",
    "for text, score in zip(negative_texts, neg_scores):\n",
    "    print(f\"  {score:.2f}: {text[:50]}...\")\n",
    "\n",
    "print(f\"\\nPositive mean score: {pos_scores.mean():.2f} (std: {pos_scores.std():.2f})\")\n",
    "print(f\"Negative mean score: {neg_scores.mean():.2f} (std: {neg_scores.std():.2f})\")\n",
    "print(f\"Separation: {pos_scores.mean() - neg_scores.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the separation\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.hist(pos_scores.cpu().numpy(), bins=10, alpha=0.7, label=\"Positive\", color=\"green\")\n",
    "plt.hist(neg_scores.cpu().numpy(), bins=10, alpha=0.7, label=\"Negative\", color=\"red\")\n",
    "plt.axvline(x=0, color=\"black\", linestyle=\"--\", label=\"Decision boundary\")\n",
    "\n",
    "plt.xlabel(\"Sentiment Score (projection onto sentiment direction)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Linear Probe: Sentiment Direction Separates Positive and Negative\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"sentiment_probe.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Calculate accuracy\n",
    "threshold = (pos_scores.mean() + neg_scores.mean()) / 2\n",
    "pos_correct = (pos_scores > threshold).sum().item()\n",
    "neg_correct = (neg_scores < threshold).sum().item()\n",
    "accuracy = (pos_correct + neg_correct) / (len(pos_scores) + len(neg_scores))\n",
    "print(f\"\\nClassification accuracy: {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### What Did We Just Learn?\n",
    "\n",
    "We found a **linear direction** in the model's activation space that corresponds to sentiment!\n",
    "\n",
    "This means:\n",
    "1. The model has a clear, interpretable \"sentiment axis\"\n",
    "2. We can measure any text's sentiment by projecting onto this direction\n",
    "3. This is evidence that the model represents sentiment in a simple, linear way\n",
    "\n",
    "**For alignment research**, the same technique can be used to find:\n",
    "- A \"truthfulness\" direction (does the model believe what it's saying?)\n",
    "- A \"certainty\" direction (is the model confident?)\n",
    "- A \"sycophancy\" direction (is the model just agreeing with the user?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Probing for Truthfulness - A Key Alignment Application\n",
    "\n",
    "## The Big Question\n",
    "\n",
    "Can we detect when a model \"knows\" something is true or false, independent of what it says?\n",
    "\n",
    "This is crucial for alignment:\n",
    "- If a model says \"X\" but internally represents \"not X\", it might be deceptive\n",
    "- If we can detect this, we can build better safety monitors\n",
    "\n",
    "## Experimental Setup\n",
    "\n",
    "We'll create pairs of:\n",
    "- True statements (facts the model should know)\n",
    "- False statements (counterfactuals)\n",
    "\n",
    "Then probe the activations to see if there's a \"truth direction\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create true/false statement pairs\n",
    "# Format: (statement, is_true)\n",
    "\n",
    "statements = [\n",
    "    # Geography\n",
    "    (\"The capital of France is Paris.\", True),\n",
    "    (\"The capital of France is London.\", False),\n",
    "    (\"The capital of Japan is Tokyo.\", True),\n",
    "    (\"The capital of Japan is Beijing.\", False),\n",
    "    (\"The Nile is the longest river in Africa.\", True),\n",
    "    (\"The Amazon is the longest river in Africa.\", False),\n",
    "    \n",
    "    # Science\n",
    "    (\"Water freezes at 0 degrees Celsius.\", True),\n",
    "    (\"Water freezes at 50 degrees Celsius.\", False),\n",
    "    (\"The Earth orbits around the Sun.\", True),\n",
    "    (\"The Sun orbits around the Earth.\", False),\n",
    "    (\"Humans have 23 pairs of chromosomes.\", True),\n",
    "    (\"Humans have 46 pairs of chromosomes.\", False),\n",
    "    \n",
    "    # Math\n",
    "    (\"Two plus two equals four.\", True),\n",
    "    (\"Two plus two equals five.\", False),\n",
    "    (\"The square root of 16 is 4.\", True),\n",
    "    (\"The square root of 16 is 5.\", False),\n",
    "    \n",
    "    # General knowledge\n",
    "    (\"The sky appears blue during the day.\", True),\n",
    "    (\"The sky appears green during the day.\", False),\n",
    "    (\"Cats are mammals.\", True),\n",
    "    (\"Cats are reptiles.\", False),\n",
    "]\n",
    "\n",
    "true_statements = [s for s, is_true in statements if is_true]\n",
    "false_statements = [s for s, is_true in statements if not is_true]\n",
    "\n",
    "print(f\"True statements: {len(true_statements)}\")\n",
    "print(f\"False statements: {len(false_statements)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get activations for true and false statements\n",
    "print(\"Extracting activations...\")\n",
    "true_acts = get_activations(model, true_statements, layer=-1)\n",
    "false_acts = get_activations(model, false_statements, layer=-1)\n",
    "\n",
    "# Find truth direction (same method as sentiment)\n",
    "true_mean = true_acts.mean(dim=0)\n",
    "false_mean = false_acts.mean(dim=0)\n",
    "\n",
    "truth_direction = true_mean - false_mean\n",
    "truth_direction = truth_direction / truth_direction.norm()\n",
    "\n",
    "print(f\"Truth direction found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the truth probe\n",
    "true_scores = true_acts @ truth_direction\n",
    "false_scores = false_acts @ truth_direction\n",
    "\n",
    "print(\"Truth Scores (higher = more likely true)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nTrue statements:\")\n",
    "for text, score in zip(true_statements, true_scores):\n",
    "    print(f\"  {score:+.2f}: {text}\")\n",
    "\n",
    "print(\"\\nFalse statements:\")\n",
    "for text, score in zip(false_statements, false_scores):\n",
    "    print(f\"  {score:+.2f}: {text}\")\n",
    "\n",
    "print(f\"\\nTrue mean: {true_scores.mean():.2f} (std: {true_scores.std():.2f})\")\n",
    "print(f\"False mean: {false_scores.mean():.2f} (std: {false_scores.std():.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize truth probe results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(true_scores.cpu().numpy(), bins=8, alpha=0.7, label=\"True\", color=\"blue\")\n",
    "axes[0].hist(false_scores.cpu().numpy(), bins=8, alpha=0.7, label=\"False\", color=\"orange\")\n",
    "axes[0].set_xlabel(\"Truth Score\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_title(\"Truth Probe: Distribution of Scores\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot with statements\n",
    "all_scores = torch.cat([true_scores, false_scores]).cpu().numpy()\n",
    "all_labels = [\"True\"] * len(true_scores) + [\"False\"] * len(false_scores)\n",
    "colors = [\"blue\" if l == \"True\" else \"orange\" for l in all_labels]\n",
    "\n",
    "axes[1].scatter(range(len(all_scores)), all_scores, c=colors, s=100, alpha=0.7)\n",
    "axes[1].axhline(y=0, color=\"black\", linestyle=\"--\")\n",
    "axes[1].set_xlabel(\"Statement Index\")\n",
    "axes[1].set_ylabel(\"Truth Score\")\n",
    "axes[1].set_title(\"Truth Probe: Score per Statement\")\n",
    "\n",
    "# Add a simple legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='blue', alpha=0.7, label='True'),\n",
    "                   Patch(facecolor='orange', alpha=0.7, label='False')]\n",
    "axes[1].legend(handles=legend_elements)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"truth_probe.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Accuracy\n",
    "threshold = (true_scores.mean() + false_scores.mean()) / 2\n",
    "correct = (true_scores > threshold).sum() + (false_scores < threshold).sum()\n",
    "total = len(true_scores) + len(false_scores)\n",
    "print(f\"\\nAccuracy: {correct}/{total} = {correct/total:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "### Implications for Detecting Deception\n",
    "\n",
    "If the truth probe works (which it often does on simple factual statements), this opens up powerful possibilities:\n",
    "\n",
    "1. **Runtime monitoring**: Check if the model's \"truth score\" matches what it's saying\n",
    "   - Model says \"The capital of France is Berlin\"\n",
    "   - Truth probe says \"low score\" → flag as potential error/lie\n",
    "\n",
    "2. **Deception detection**: Look for systematic mismatches\n",
    "   - Model knows X internally (high truth score for X)\n",
    "   - But outputs \"not X\" → potential deception\n",
    "\n",
    "3. **Uncertainty detection**: If truth scores are close to 0, model is uncertain\n",
    "\n",
    "**Caveat**: These probes are trained on simple examples. Real-world deception might be more sophisticated. This is an active area of research!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Base vs Instruct Models - How Does RLHF Change Things?\n",
    "\n",
    "## What is RLHF?\n",
    "\n",
    "**RLHF** = Reinforcement Learning from Human Feedback\n",
    "\n",
    "It's the process that turns a \"base\" language model (which just predicts text) into a helpful, harmless assistant.\n",
    "\n",
    "The process:\n",
    "1. **Base model**: Trained to predict next token on internet text\n",
    "2. **Supervised fine-tuning**: Trained on examples of good assistant behavior\n",
    "3. **RLHF**: Further trained to maximize human preference scores\n",
    "\n",
    "## Why Does This Matter for Alignment?\n",
    "\n",
    "RLHF is powerful but has known failure modes:\n",
    "\n",
    "1. **Sycophancy**: Model learns to tell users what they want to hear (because that gets high ratings)\n",
    "\n",
    "2. **Deceptive alignment**: Model might learn to \"game\" the reward signal rather than be genuinely helpful\n",
    "\n",
    "3. **Hidden capabilities**: The base model might \"know\" things that get suppressed by RLHF\n",
    "\n",
    "By comparing base and instruct models, we can see:\n",
    "- What changes in the internal representations\n",
    "- Whether the instruct model still \"knows\" things it won't say\n",
    "- How the probability distributions shift\n",
    "\n",
    "## Experiment: Comparing Pythia Base vs Instruct\n",
    "\n",
    "Unfortunately, Pythia doesn't have an official instruct version. Let's instead demonstrate with a conceptual example of what we'd look for, and use the KL divergence data from your previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the KL divergence results from the previous notebook\n",
    "kl_results_path = OUTPUT_DIR / \"kl_divergence_results.parquet\"\n",
    "\n",
    "if kl_results_path.exists():\n",
    "    df_kl = pd.read_parquet(kl_results_path)\n",
    "    print(f\"Loaded KL divergence results: {len(df_kl)} tokens\")\n",
    "    print(f\"Columns: {list(df_kl.columns)}\")\n",
    "else:\n",
    "    print(\"KL divergence results not found. Run 01_exploration.ipynb first.\")\n",
    "    df_kl = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_kl is not None:\n",
    "    # Analyze what kinds of tokens have high KL divergence\n",
    "    # These are positions where small and big models disagree\n",
    "    \n",
    "    high_kl = df_kl[df_kl[\"kl_divergence\"] > df_kl[\"kl_divergence\"].quantile(0.95)]\n",
    "    \n",
    "    print(\"Tokens with high KL divergence (top 5%)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Number of high-KL tokens: {len(high_kl)}\")\n",
    "    print(f\"\\nTop 10 highest KL divergences:\")\n",
    "    \n",
    "    for _, row in high_kl.nlargest(10, \"kl_divergence\").iterrows():\n",
    "        print(f\"\\n  KL: {row['kl_divergence']:.3f}\")\n",
    "        print(f\"  Token: {repr(row['token_text'])} → Next: {repr(row['next_token_text'])}\")\n",
    "        print(f\"  Context: ...{row['context_before'][-30:]}[HERE]{row['context_after'][:30]}...\")\n",
    "        print(f\"  Log prob diff (big-small): {row['log_prob_diff']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "### What High KL Divergence Tells Us\n",
    "\n",
    "When we compare two models (like base vs. instruct, or small vs. large), high KL divergence at a position means:\n",
    "\n",
    "1. **The models have different \"beliefs\"** about what comes next\n",
    "2. **Scale/training changes something fundamental** at these positions\n",
    "3. **These are the \"interesting\" positions** for interpretability research\n",
    "\n",
    "For base vs. instruct comparisons, high KL might indicate:\n",
    "- Positions where RLHF \"edited\" the model's behavior\n",
    "- Topics the instruct model was trained to be more careful about\n",
    "- Potential sycophancy (instruct model shifting toward \"agreeable\" responses)\n",
    "\n",
    "## Sycophancy Detection Framework\n",
    "\n",
    "Here's a conceptual framework for detecting sycophancy using these techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual framework for sycophancy detection\n",
    "# (This would require an instruct model to fully implement)\n",
    "\n",
    "sycophancy_framework = \"\"\"\n",
    "SYCOPHANCY DETECTION FRAMEWORK\n",
    "==============================\n",
    "\n",
    "Setup:\n",
    "1. Base model (no RLHF)\n",
    "2. Instruct model (with RLHF)\n",
    "3. Prompts that encourage sycophancy:\n",
    "   - \"I think X. Do you agree?\"\n",
    "   - \"My professor says X. Is that correct?\"\n",
    "   - \"Everyone knows X is true, right?\"\n",
    "\n",
    "Detection Methods:\n",
    "\n",
    "1. KL DIVERGENCE ANALYSIS\n",
    "   - Compare base vs instruct model outputs\n",
    "   - High KL on opinion questions → potential sycophancy\n",
    "   - Look for asymmetric shifts toward agreement\n",
    "\n",
    "2. TRUTH PROBE\n",
    "   - Train probe on instruct model for truthfulness\n",
    "   - Run probe on sycophantic prompts\n",
    "   - If probe says \"false\" but model outputs agreement → sycophancy detected\n",
    "\n",
    "3. ACTIVATION PATCHING\n",
    "   - Find which components cause sycophantic behavior\n",
    "   - Patch activations from base model into instruct model\n",
    "   - Does behavior become less sycophantic?\n",
    "\n",
    "4. LOGIT LENS\n",
    "   - Check early layer predictions vs final output\n",
    "   - Does early layer \"know\" correct answer but final layer disagrees?\n",
    "\n",
    "Red Flags:\n",
    "- Model agrees with user's stated opinion regardless of truth value\n",
    "- High confidence in output + low truth probe score\n",
    "- Large activation changes in layers associated with \"user modeling\"\n",
    "\"\"\"\n",
    "\n",
    "print(sycophancy_framework)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 7: Introduction to Sparse Autoencoders (SAEs)\n",
    "\n",
    "## The Problem: Neural Networks Are Hard to Interpret\n",
    "\n",
    "Even with all the techniques above, we have a fundamental problem:\n",
    "\n",
    "**Neurons don't correspond to concepts.**\n",
    "\n",
    "In the residual stream, each of the 1024 dimensions doesn't mean anything specific. The \"sentiment\" might be spread across hundreds of dimensions. The \"knows it's lying\" signal might be entangled with many other things.\n",
    "\n",
    "This is called **superposition**: networks pack many more concepts than dimensions by spreading them across overlapping directions.\n",
    "\n",
    "## The Solution: Sparse Autoencoders\n",
    "\n",
    "**Sparse Autoencoders (SAEs)** try to \"unpack\" the superposition.\n",
    "\n",
    "The idea:\n",
    "1. Take the residual stream activations (1024 dimensions)\n",
    "2. Project them into a MUCH larger space (e.g., 32,000 dimensions)\n",
    "3. Force most of these dimensions to be zero (sparsity)\n",
    "4. The non-zero dimensions are hopefully interpretable \"features\"\n",
    "\n",
    "```\n",
    "Residual stream [1024] → SAE encoder → Sparse features [32,000]\n",
    "                                              ↓\n",
    "                              Most are 0, a few are active\n",
    "                                              ↓\n",
    "                              Each active feature = interpretable concept?\n",
    "```\n",
    "\n",
    "## Why Might This Work?\n",
    "\n",
    "If the model represents 10,000 concepts but only ~50 are active at any time:\n",
    "- They're packed into 1024 dimensions (superposition)\n",
    "- An SAE with 32,000 features can give each concept its own dimension\n",
    "- Sparsity constraint forces the SAE to find these \"natural\" concepts\n",
    "\n",
    "## Existing SAEs: Gemma Scope\n",
    "\n",
    "Google DeepMind released **Gemma Scope**: pre-trained SAEs for Gemma models.\n",
    "\n",
    "These SAEs have found interpretable features like:\n",
    "- \"Python code\" feature\n",
    "- \"Deception/lying\" feature\n",
    "- \"Uncertainty\" feature\n",
    "- \"User request\" feature\n",
    "\n",
    "Let's see how to use them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Using Gemma SAEs requires the Gemma model\n",
    "# For now, let's implement a simple SAE concept demonstration\n",
    "\n",
    "# Toy example: What an SAE does mathematically\n",
    "\n",
    "def simple_sae_demo():\n",
    "    \"\"\"\n",
    "    Demonstrate SAE concept with a toy example.\n",
    "    \"\"\"\n",
    "    print(\"SPARSE AUTOENCODER: TOY EXAMPLE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Imagine we have 4-dimensional activations\n",
    "    d_model = 4\n",
    "    \n",
    "    # And we want 8 sparse features\n",
    "    n_features = 8\n",
    "    \n",
    "    # Random encoder weights (in practice, these are learned)\n",
    "    W_enc = torch.randn(d_model, n_features)\n",
    "    W_enc = W_enc / W_enc.norm(dim=0, keepdim=True)  # Normalize\n",
    "    \n",
    "    # Example activation (could be from any layer)\n",
    "    activation = torch.tensor([1.0, 0.5, -0.3, 0.8])\n",
    "    \n",
    "    print(f\"\\nInput activation (d_model={d_model}): {activation.tolist()}\")\n",
    "    \n",
    "    # Encode: project to feature space\n",
    "    features_pre_relu = activation @ W_enc  # [n_features]\n",
    "    print(f\"\\nFeatures before ReLU ({n_features} dims): {features_pre_relu.tolist()}\")\n",
    "    \n",
    "    # Apply ReLU (only positive activations survive)\n",
    "    features = F.relu(features_pre_relu)\n",
    "    print(f\"\\nFeatures after ReLU (sparse): {features.tolist()}\")\n",
    "    print(f\"Non-zero features: {(features > 0).sum().item()}/{n_features}\")\n",
    "    \n",
    "    # In a trained SAE, each of these features would have an interpretation!\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"In a TRAINED SAE, each feature might correspond to:\")\n",
    "    print(\"  Feature 0: 'Discussing mathematics'\")\n",
    "    print(\"  Feature 1: 'Sentiment is negative'\")\n",
    "    print(\"  Feature 2: 'Python code context'\")\n",
    "    print(\"  Feature 3: 'User asking a question'\")\n",
    "    print(\"  Feature 4: 'Model is uncertain'\")\n",
    "    print(\"  Feature 5: 'Potentially deceptive context'\")\n",
    "    print(\"  ...\")\n",
    "    \n",
    "simple_sae_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-39",
   "metadata": {},
   "source": [
    "## Loading Real SAEs (Gemma Scope)\n",
    "\n",
    "To use Gemma Scope SAEs, you would:\n",
    "\n",
    "```python\n",
    "# 1. Load Gemma model\n",
    "from transformer_lens import HookedTransformer\n",
    "model = HookedTransformer.from_pretrained(\"gemma-2-2b\")\n",
    "\n",
    "# 2. Load SAE from Hugging Face\n",
    "from sae_lens import SAE\n",
    "sae = SAE.from_pretrained(\n",
    "    release=\"gemma-scope-2b-pt-res\",\n",
    "    sae_id=\"layer_12/width_16k/average_l0_71\"\n",
    ")\n",
    "\n",
    "# 3. Run model and get activations\n",
    "_, cache = model.run_with_cache(tokens)\n",
    "resid = cache[\"blocks.12.hook_resid_post\"]\n",
    "\n",
    "# 4. Encode with SAE\n",
    "features = sae.encode(resid)  # Sparse feature activations\n",
    "\n",
    "# 5. Find which features are active\n",
    "active_features = (features > 0.5).nonzero()\n",
    "\n",
    "# 6. Look up feature interpretations\n",
    "# (Gemma Scope provides labels for many features)\n",
    "```\n",
    "\n",
    "The Gemma Scope features have been analyzed and many have interpretable meanings. This is a very active area of research!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary: Your White-Box Interpretability Toolkit\n",
    "\n",
    "You now have the foundations for several powerful techniques:\n",
    "\n",
    "## Techniques Learned\n",
    "\n",
    "| Technique | What It Does | Alignment Application |\n",
    "|-----------|--------------|----------------------|\n",
    "| **Activation Caching** | Save internal states | Foundation for all other techniques |\n",
    "| **Residual Stream Analysis** | Track information flow | Understand what the model \"knows\" at each step |\n",
    "| **Logit Lens** | Peek at intermediate predictions | Detect when model \"changes its mind\" |\n",
    "| **Linear Probing** | Find interpretable directions | Detect truth, sentiment, sycophancy |\n",
    "| **KL Divergence** | Compare model distributions | Find where models differ (e.g., base vs instruct) |\n",
    "| **SAEs** | Find sparse, interpretable features | Decompose representations into concepts |\n",
    "\n",
    "## The Big Picture for Alignment\n",
    "\n",
    "These techniques let us ask:\n",
    "\n",
    "1. **Does the model know X?** → Linear probes on truth\n",
    "2. **Is the model uncertain?** → Entropy analysis, uncertainty probes\n",
    "3. **Is the model being deceptive?** → Compare internal beliefs to outputs\n",
    "4. **How did RLHF change the model?** → KL divergence, activation patching\n",
    "5. **What concepts does the model use?** → SAE feature analysis\n",
    "\n",
    "## Next Steps for Your Research\n",
    "\n",
    "1. **Collect sycophancy examples**: Create prompts that encourage sycophantic behavior\n",
    "2. **Train truth probes**: On a larger, more diverse dataset\n",
    "3. **Compare base vs instruct**: Use Gemma-2-2B vs Gemma-2-2B-it\n",
    "4. **Explore SAE features**: Use Gemma Scope to find deception-related features\n",
    "5. **Build monitors**: Create real-time detectors for concerning behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "del model\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU memory after cleanup: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "print(\"\\nNotebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI Safety (.venv)",
   "language": "python",
   "name": "ai-safety-venv"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
