# Cross-Modal Semantics in Gemma 3: Do "Cat" (Text) and "Cat" (Image) Share a Feature?

## Self-Directed Learning Project in Mechanistic Interpretability

**Theme:** Frontier Model Biology / Multimodal
**Difficulty:** High (Novelty)
**Format:** 20-hour research sprint

---

## Executive Summary

This project investigates whether multimodal models learn **"Platonic" representations**â€”abstract concept features that are shared across modalities. Using Gemma Scope 2 on the multimodal Gemma 3 family, we'll search for SAE features that activate robustly for both text descriptions and image embeddings of the same concept, mapping the geometry of cross-modal integration.

---

## Why This Research Matters

### The Core Question

Do multimodal models:
- **A) Learn separate representations** for text "cat" and image of a cat?
- **B) Learn unified "Platonic" concepts** that fire regardless of modality?

Understanding this has profound implications for:
- How concepts are represented in modern AI systems
- Whether safety interventions on text transfer to images
- The fundamental architecture of multimodal understanding

### Why Neel Nanda is Investing Here

With Gemma Scope 2 covering the multimodal Gemma 3 family, Nanda is explicitly interested in research that "teaches him something new" about these brand-new architectures. Key questions:
- How does multimodal integration actually work internally?
- Are there interpretable cross-modal features?
- What is the "biology" of multimodal models?

This is **basic science on the absolute cutting edge**.

### Alignment with Pragmatic Interpretability

| Research Direction | How This Project Addresses It |
|-------------------|------------------------------|
| **Frontier Model Biology** | First interpretability study on Gemma 3 multimodal |
| **Gemma Scope 2** | Uses DeepMind's latest SAEs on newest architecture |
| **Basic Science** | Investigates fundamental questions about representation |
| **Novel Contribution** | High chance of discovering something new |

---

## Core Hypothesis

**Hypothesis:** There exist "Abstract Concept Features" in late layers of Gemma 3 that fire robustly for both the text token "Eiffel Tower" and an image embedding of the Eiffel Tower.

### The Platonic Representation Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PLATONIC vs. MODALITY-SPECIFIC MODEL                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  MODALITY-SPECIFIC (Hypothesis A)                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚                                          â”‚                   â”‚
â”‚  â”‚  Text: "cat" â”€â”€â–º Text-Cat Feature        â”‚                   â”‚
â”‚  â”‚                      â†“                   â”‚                   â”‚
â”‚  â”‚                  [Separate]              â”‚                   â”‚
â”‚  â”‚                      â†‘                   â”‚                   â”‚
â”‚  â”‚  Image: ğŸ± â”€â”€â”€â–º Image-Cat Feature        â”‚                   â”‚
â”‚  â”‚                                          â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                                  â”‚
â”‚  PLATONIC (Hypothesis B - What We're Testing)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚                                          â”‚                   â”‚
â”‚  â”‚  Text: "cat" â”€â”€â”                         â”‚                   â”‚
â”‚  â”‚                â”œâ”€â”€â–º Abstract "Cat"       â”‚                   â”‚
â”‚  â”‚  Image: ğŸ± â”€â”€â”€â”˜     Feature              â”‚                   â”‚
â”‚  â”‚                                          â”‚                   â”‚
â”‚  â”‚  (Modality-invariant representation)     â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Testable Predictions

1. **Shared Features Exist**: SAE features with high activation for both text and image inputs of same concept
2. **High Mutual Information**: Significant correlation between text and image activations for same concept
3. **Late Layer Convergence**: Cross-modal features more prevalent in later layers
4. **Causal Cross-Modal Steering**: Clamping text-derived features affects image interpretation

---

## Technical Approach

### Methodology

#### Phase 1: Dataset Creation

| Concept Category | Text Input | Image Input | Count |
|-----------------|------------|-------------|-------|
| **Landmarks** | "Eiffel Tower", "Statue of Liberty" | Photos | 20 |
| **Animals** | "cat", "elephant", "penguin" | Photos | 20 |
| **Objects** | "red apple", "vintage car" | Photos | 20 |
| **Scenes** | "beach sunset", "snowy mountain" | Photos | 20 |
| **Abstract** | "happiness", "danger" | Evocative images | 10 |

**Critical Design Choice**: Match text descriptions exactly to image content to ensure fair comparison.

#### Phase 2: Cross-Modal Feature Search

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CROSS-MODAL FEATURE SEARCH PIPELINE                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  INPUT PAIRS: (Text, Image) for same concept                    â”‚
â”‚                                                                  â”‚
â”‚  For each pair:                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚  1. Run text through Gemma 3            â”‚                    â”‚
â”‚  â”‚  2. Run image through Gemma 3           â”‚                    â”‚
â”‚  â”‚  3. Extract SAE activations (late layers)â”‚                   â”‚
â”‚  â”‚  4. Compute feature overlap             â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                                                  â”‚
â”‚  METRICS:                                                       â”‚
â”‚  â€¢ Jaccard similarity of active features                        â”‚
â”‚  â€¢ Correlation of feature activation strengths                  â”‚
â”‚  â€¢ Mutual information between modality activations              â”‚
â”‚                                                                  â”‚
â”‚  OUTPUT: Cross-modal feature candidates                         â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Phase 3: Validation and Intervention

| Experiment | Method | Expected Result |
|------------|--------|-----------------|
| **Concept Specificity** | Do "cat" features fire for cat images but not dog images? | High specificity |
| **Cross-Modal Steering** | Clamp "snow" feature (from text); describe beach image | Model mentions snow/cold? |
| **Layer Analysis** | Compare cross-modal overlap across layers | More overlap in late layers |
| **Baseline Comparison** | Compare random text-image pairs | Low overlap (control) |

### Tooling Stack

| Tool | Purpose |
|------|---------|
| **Gemma 3** | Multimodal target model |
| **Gemma Scope 2** | Pre-trained SAEs for Gemma 3 |
| **TransformerLens** | Model hooking (if compatible) |
| **SAELens** | SAE loading and analysis |
| **Custom scripts** | Multimodal activation extraction |

---

## Success Metrics

### Primary Metric: Cross-Modal Feature Correlation

```
CrossModal_Score = Correlation(SAE_features(text), SAE_features(image))
                   for matched concept pairs
```

### Experiment Success Criteria

| Experiment | Success Criterion |
|------------|-------------------|
| Feature Identification | â‰¥10 features with >0.6 cross-modal correlation |
| Concept Specificity | Cross-modal features are concept-specific (not generic activation) |
| Steering Effect | Cross-modal steering produces measurable output change |
| Layer Gradient | Clear increase in cross-modal overlap from early to late layers |

### Paper-Ready Outcomes

| Outcome Level | Definition |
|---------------|------------|
| **Minimum** | Document cross-modal activation patterns; methodology |
| **Target** | Identify candidate Platonic features; demonstrate steering |
| **Stretch** | Map geometry of multimodal integration; contribute to Gemma 3 interpretability |

---

## Detailed Experimental Design

### Experiment 1: Cross-Modal Correlation Analysis

For each concept:
1. Collect top-50 active SAE features for text input
2. Collect top-50 active SAE features for image input
3. Compute overlap and correlation

```
Concept         | Text Features | Image Features | Overlap | Correlation
----------------|---------------|----------------|---------|------------
Eiffel Tower    | [f1, f2, ...]| [f3, f4, ...]  | ___     | ___
Cat             | [...]         | [...]          | ___     | ___
Beach Sunset    | [...]         | [...]          | ___     | ___
```

### Experiment 2: Specificity Testing

Do "cat" features fire for dog images?

```
Feature: "Cat-candidate-feature-42"

Activation on:
- Text "cat": ___
- Image of cat: ___
- Text "dog": ___
- Image of dog: ___

Specificity = (cat_activation) / (cat_activation + dog_activation)
```

### Experiment 3: Cross-Modal Steering

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CROSS-MODAL STEERING EXPERIMENT                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  SETUP:                                                         â”‚
â”‚  â€¢ Input: Image of sunny beach                                  â”‚
â”‚  â€¢ Task: "Describe this image"                                  â”‚
â”‚                                                                  â”‚
â”‚  BASELINE: Normal description                                   â”‚
â”‚  "A beautiful sunny beach with blue water and sand..."          â”‚
â”‚                                                                  â”‚
â”‚  INTERVENTION: Clamp "Snow" feature (derived from text)         â”‚
â”‚  Expected: Description mentions cold/winter elements?           â”‚
â”‚  "A beach scene... the sand looks almost like snow..."          â”‚
â”‚                                                                  â”‚
â”‚  SUCCESS: Text-derived features causally affect image processingâ”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Connection to AI Safety

### Why Cross-Modal Representations Matter for Safety

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SAFETY IMPLICATIONS                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  IF concepts are shared across modalities:                      â”‚
â”‚                                                                  â”‚
â”‚  âœ“ Safety interventions on text may transfer to images          â”‚
â”‚  âœ“ A "violence" feature ablated from text also blocks           â”‚
â”‚    violent image generation/interpretation                       â”‚
â”‚  âœ“ Monitoring systems can use unified detectors                 â”‚
â”‚                                                                  â”‚
â”‚  IF concepts are modality-specific:                             â”‚
â”‚                                                                  â”‚
â”‚  âœ— Need separate safety measures for each modality              â”‚
â”‚  âœ— Jailbreaks via modality switching may be easier              â”‚
â”‚  âœ— More complex monitoring requirements                         â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implications for Multimodal Safety

1. **Transfer of Safety Training**: Do text-based safety interventions protect against image-based attacks?
2. **Unified Monitoring**: Can we build one monitor for all modalities?
3. **Attack Surface**: Are cross-modal inconsistencies exploitable?
4. **Representation Alignment**: Are multimodal models more or less aligned across modalities?

---

## Risks and Mitigations

| Risk | Level | Mitigation |
|------|-------|------------|
| **Gemma 3 SAEs not yet available** | Medium | Use available layers; contribute early findings |
| **Architecture complexity** | High | Focus on late layers where integration happens |
| **No cross-modal features found** | Medium | Important negative result; may indicate modality-specific processing |
| **Tooling compatibility** | Medium | May need custom activation extraction |

---

## Research Questions

### Primary Question
**Do multimodal models learn unified "Platonic" concept representations that are shared across text and image modalities?**

### Sub-Questions

1. **Feature Existence**: Are there SAE features that fire for both text and image inputs of the same concept?
2. **Layer Distribution**: At which layers does cross-modal integration occur?
3. **Causal Role**: Do text-derived features causally influence image processing (and vice versa)?
4. **Geometry**: What is the structure of the multimodal representation space?

---

## Key References

1. **Gemma Scope 2** - DeepMind (2025)
2. **"The Platonic Representation Hypothesis"** - Recent multimodal work
3. **CLIP and Vision-Language Models** - Cross-modal representation literature
4. **Gemma 3 Technical Report** - Google DeepMind
5. **A Pragmatic Vision for Interpretability** - Neel Nanda

---

## Timeline (20-Hour Sprint)

| Hours | Focus | Deliverables |
|-------|-------|--------------|
| 1-3 | Dataset Creation | 90+ matched text-image pairs |
| 4-8 | Activation Extraction | Cross-modal feature activations |
| 9-14 | Correlation Analysis | Cross-modal feature candidates |
| 15-18 | Steering Experiments | Causal verification |
| 19-20 | Write-up | Findings; geometry visualization; implications |

---

## Project Status

**Status**: Research idea documented
**Next Steps**: Verify Gemma Scope 2 availability for Gemma 3; begin dataset curation

---

*Project initialized: January 2026*
